## CouchDB Scripts Guide

This directory contains four script files:

1. **data_twi_mpi.py**: This script is responsible for uploading preprocessed data (sport_data) to the CouchDB on the main-server using multi-threading.

2. **data_push_external.py**: This script uploads preprocessed external sudo data to the CouchDB on the main-server.

3. **data_map_reduce.py**: This script performs mapreduce operations on Twitter data and external data on the twi-server.

4. **data_push_mapreduce.py**: This script returns the results of the mapreduce operations to CouchDB and generates corresponding new database views.

The execution commands for `data_twi_mpi.py`, `data_push_external.py`, and `data_map_reduce.py` have been added to the `replicate_database.sh` file. 

To use, log into the virtual machine of the main-server, cd to the directory containing the scripts, and execute the `./replicate_database.sh` command.

After all three scripts have been executed, log into the CouchDB on the twi-server virtual machine, and ensure that all mapreduce views have been generated. Then, run the `python3 data_push_mapreduce.py` command. This will store the views generated by the mapreduce operation in their respective databases.
